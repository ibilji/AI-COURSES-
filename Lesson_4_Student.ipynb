{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5789bc3-b1ae-42c7-94a8-2ef4f89946fc",
   "metadata": {},
   "source": [
    "# Lesson 4: Persistence and Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5762271-8736-4e94-9444-8c92bd0e8074",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0168aee-bce9-4d60-b827-f86a88187e31",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da06a64f-a2d5-4a66-8090-9ada0930c684",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "tool = TavilySearchResults(max_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589c5b6-6cc2-4594-9a17-dccdcf676054",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c033522-d2fc-41ac-8e3c-5e35872bf88d",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba84ec-c172-4de7-ac55-e3158a531b23",
   "metadata": {
    "height": 574
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, checkpointer, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d5092-b8ef-4e38-b4d7-0e80c609bf7a",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10084a02-2928-4945-9f7c-ad3f5b33caf7",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in sf?\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714d1205-f8fc-4912-b148-2a45da99219c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83588e70-254f-4f83-a510-c8ae81e729b0",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb3ef4c-58b3-401b-b104-0d51e553d982",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What about in la?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3293b7-a50c-43c8-a022-8975e1e444b8",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722c3d4-4cbf-43bf-81b0-50f634c4ce61",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace59a36-3941-459e-b9d1-ac5a4a1ed3ae",
   "metadata": {},
   "source": [
    "## Streaming tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f82fe-3ec4-4917-be51-9fb10d1317fa",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
    "\n",
    "memory = AsyncSqliteSaver.from_conn_string(\":memory:\")\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0fe1c7-77e2-499c-a2f9-1f739bb6ddf0",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in SF?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "async for event in abot.graph.astream_events({\"messages\": messages}, thread, version=\"v1\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        content = event[\"data\"][\"chunk\"].content\n",
    "        if content:\n",
    "            # Empty content in the context of OpenAI means\n",
    "            # that the model is asking for a tool to be invoked.\n",
    "            # So we only print non-empty content\n",
    "            print(content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f303b1-a4d0-408c-8cc0-515ff980717f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "650b4032",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dea9b1cb",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4172f4f7",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "tool = TavilySearchResults(max_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2caa97d",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c10dd4f",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f4ab6a9",
   "metadata": {
    "height": 591
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, checkpointer, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17183d63",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa7bead6",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in sf?\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f43df1f6",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "380c4d40",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_bmfLa92f6oAIKN9KvXtqbKDz', 'function': {'arguments': '{\"query\":\"current weather in San Francisco\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 151, 'total_tokens': 173, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_831e067d82', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-ad5687a3-44c1-4c18-872a-a6d7861af98f-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'current weather in San Francisco'}, 'id': 'call_bmfLa92f6oAIKN9KvXtqbKDz'}])]\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'current weather in San Francisco'}, 'id': 'call_bmfLa92f6oAIKN9KvXtqbKDz'}\n",
      "Back to the model!\n",
      "[ToolMessage(content='[{\\'url\\': \\'https://www.weather25.com/north-america/usa/california/san-francisco?page=month&month=July\\', \\'content\\': \\'weather25.com\\\\nSearch\\\\nweather in United States\\\\nRemove from your favorite locations\\\\nAdd to my locations\\\\nShare\\\\nweather in United States\\\\n\\\\n# San Francisco weather in July 2025\\\\n\\\\nOvercast\\\\nOvercast\\\\nMist\\\\nPatchy rain possible\\\\nOvercast\\\\nClear\\\\nClear\\\\nSunny\\\\nClear\\\\nClear\\\\nClear\\\\nSunny\\\\nSunny\\\\nSunny\\\\n\\\\n## The average weather in San Francisco in July\\\\n\\\\nThe temperatures in San Francisco in July are comfortable with low of 14Â°C and and high up to 25Â°C. [...] | 13 Partly cloudy 19Â° /12Â° | 14 Partly cloudy 20Â° /13Â° | 15 Partly cloudy 20Â° /13Â° | 16 Sunny 20Â° /13Â° | 17 Overcast 15Â° /14Â° | 18 Overcast 15Â° /13Â° | 19 Mist 14Â° /12Â° |\\\\n| 20 Patchy rain possible 17Â° /13Â° | 21 Overcast 18Â° /14Â° | 22 Sunny 17Â° /14Â° | 23 Sunny 18Â° /13Â° | 24 Sunny 17Â° /13Â° | 25 Sunny 17Â° /14Â° | 26 Sunny 18Â° /13Â° |\\\\n| 27 Sunny 19Â° /13Â° | 28 Sunny 21Â° /14Â° | 29 Sunny 20Â° /15Â° | 30 Sunny 19Â° /14Â° | 31 Partly cloudy 21Â° /14Â° |  |  | [...] | Sun | Mon | Tue | Wed | Thu | Fri | Sat |\\\\n| --- | --- | --- | --- | --- | --- | --- |\\\\n|  |  | 1 Cloudy 20Â° /12Â° | 2 Partly cloudy 20Â° /12Â° | 3 Sunny 20Â° /12Â° | 4 Sunny 20Â° /12Â° | 5 Sunny 20Â° /13Â° |\\\\n| 6 Sunny 19Â° /13Â° | 7 Mist 19Â° /12Â° | 8 Sunny 20Â° /12Â° | 9 Partly cloudy 20Â° /13Â° | 10 Partly cloudy 21Â° /13Â° | 11 Sunny 22Â° /13Â° | 12 Sunny 20Â° /13Â° |\\'}, {\\'url\\': \\'https://weather.com/weather/today/l/San+Francisco+CA+USCA0987:1:US\\', \\'content\\': \"## Recent Locations\\\\n\\\\nMenu\\\\n\\\\n## Weather Forecasts\\\\n\\\\n## Radar & Maps\\\\n\\\\n## News & Media\\\\n\\\\n## Products & Account\\\\n\\\\n## Lifestyle\\\\n\\\\n### Specialty Forecasts\\\\n\\\\n# San Francisco, CA\\\\n\\\\n## Small Craft Advisory\\\\n\\\\n# Hourly Weather-San Francisco, CA\\\\n\\\\n## Now\\\\n\\\\nCloudy\\\\n\\\\n## 5 am\\\\n\\\\nCloudy\\\\n\\\\n## 6 am\\\\n\\\\nCloudy\\\\n\\\\n## 7 am\\\\n\\\\nCloudy\\\\n\\\\nChart small gif\\\\n\\\\n## Don\\'t Miss\\\\n\\\\n## Seasonal Hub\\\\n\\\\n# 10 Day Weather-San Francisco, CA\\\\n\\\\n## Today\\\\n\\\\n## Day\\\\n\\\\nCloudy skies this morning will become partly cloudy this afternoon. High 66F. Winds W at 10 to 20 mph. [...] ## Night\\\\n\\\\nPartly cloudy skies this evening will become overcast overnight. Low 56F. Winds W at 10 to 20 mph.\\\\n\\\\n## Fri 18\\\\n\\\\n## Day\\\\n\\\\nCloudy skies early, followed by partial clearing. High 64F. Winds W at 15 to 25 mph.\\\\n\\\\n## Night\\\\n\\\\nPartly cloudy during the evening followed by cloudy skies overnight. Expect mist and reduced visibilities at times. Low 54F. Winds W at 10 to 20 mph.\\\\n\\\\n## Sat 19\\\\n\\\\n## Day\\\\n\\\\nCloudy early with partial sunshine expected late. High 63F. Winds W at 10 to 20 mph.\\\\n\\\\n## Night [...] Considerable cloudiness. Low 54F. Winds WSW at 10 to 20 mph.\\\\n\\\\n## Sun 20\\\\n\\\\n## Day\\\\n\\\\nIntervals of clouds and sunshine. High near 65F. Winds WSW at 10 to 20 mph.\\\\n\\\\n## Night\\\\n\\\\nClear skies with a few passing clouds. Low around 55F. Winds W at 10 to 20 mph.\\\\n\\\\n## Radar\\\\n\\\\n## Trending Now\\\\n\\\\n## We Love Our Critters\\\\n\\\\n## Summer And Your Skin\\\\n\\\\n## Home, Garage & Garden\\\\n\\\\n## Monster Firenado\\\\n\\\\n## Keeping You Healthy\\\\n\\\\n## Product Reviews & Deals\\\\n\\\\nundefined\\\\n\\\\nBeat The Glare Of A Sunny Day: TVs That Look Like Paintings\"}]', name='tavily_search_results_json', tool_call_id='call_bmfLa92f6oAIKN9KvXtqbKDz')]\n",
      "[AIMessage(content=\"The current weather in San Francisco is as follows:\\n\\n- **Daytime**: Cloudy skies this morning will become partly cloudy this afternoon with a high of 66Â°F. Winds are from the west at 10 to 20 mph.\\n- **Nighttime**: Partly cloudy skies this evening will become overcast overnight with a low of 56Â°F. Winds are from the west at 10 to 20 mph.\\n\\nOverall, it's a cloudy day with partial clearing expected later.\", response_metadata={'token_usage': {'completion_tokens': 99, 'prompt_tokens': 1225, 'total_tokens': 1324, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'model_name': 'gpt-4o', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-95330626-2d64-47d4-8ffe-56a37cec2164-0')]\n"
     ]
    }
   ],
   "source": [
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22aa02bf",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_9IKDlPoxdS3rHnNGtrw8lupx', 'function': {'arguments': '{\"query\":\"current weather in Los Angeles\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 1336, 'total_tokens': 1358, 'prompt_tokens_details': {'cached_tokens': 1280, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'model_name': 'gpt-4o', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f619a852-6722-439b-8068-eedb185eafa8-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'current weather in Los Angeles'}, 'id': 'call_9IKDlPoxdS3rHnNGtrw8lupx'}])]}\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'current weather in Los Angeles'}, 'id': 'call_9IKDlPoxdS3rHnNGtrw8lupx'}\n",
      "Back to the model!\n",
      "{'messages': [ToolMessage(content=\"[{'url': 'https://world-weather.info/forecast/usa/los_angeles/july-2025/', 'content': 'Detailed âš¡ Los Angeles Weather Forecast for July 2025 â€“ day/night ğŸŒ¡ï¸ ... Thursday, 17 July. +64Â°. Day. +79Â°. Clear sky. Friday, 18 July. +64Â°. Day. +79'}, {'url': 'https://www.weather25.com/north-america/usa/california/los-angeles?page=month&month=July', 'content': 'weather25.com\\\\nSearch\\\\nweather in United States\\\\nRemove from your favorite locations\\\\nAdd to my locations\\\\nShare\\\\nweather in United States\\\\n\\\\n# Los Angeles weather in July 2025\\\\n\\\\nPartly cloudy\\\\nPartly cloudy\\\\nPartly cloudy\\\\nClear\\\\nClear\\\\nClear\\\\nClear\\\\nClear\\\\nClear\\\\nClear\\\\nClear\\\\nPartly cloudy\\\\nPartly cloudy\\\\nClear\\\\n\\\\n## The average weather in Los Angeles in July\\\\n\\\\nThe weather in Los Angeles in July is hot. The average temperatures are between 20Â°C and 30Â°C. [...] | 13 Partly cloudy 29Â° /20Â° | 14 Sunny 29Â° /20Â° | 15 Sunny 29Â° /20Â° | 16 Sunny 29Â° /21Â° | 17 Partly cloudy 28Â° /18Â° | 18 Partly cloudy 29Â° /17Â° | 19 Partly cloudy 29Â° /17Â° |\\\\n| 20 Sunny 30Â° /20Â° | 21 Sunny 29Â° /20Â° | 22 Sunny 29Â° /20Â° | 23 Sunny 31Â° /21Â° | 24 Sunny 31Â° /22Â° | 25 Sunny 31Â° /23Â° | 26 Sunny 32Â° /23Â° |\\\\n| 27 Sunny 35Â° /24Â° | 28 Partly cloudy 38Â° /26Â° | 29 Partly cloudy 32Â° /25Â° | 30 Sunny 33Â° /24Â° | 31 Partly cloudy 31Â° /22Â° |  |  | [...] | Sun | Mon | Tue | Wed | Thu | Fri | Sat |\\\\n| --- | --- | --- | --- | --- | --- | --- |\\\\n|  |  | 1 Sunny 28Â° /19Â° | 2 Sunny 28Â° /19Â° | 3 Sunny 28Â° /19Â° | 4 Sunny 29Â° /19Â° | 5 Sunny 29Â° /19Â° |\\\\n| 6 Sunny 29Â° /20Â° | 7 Sunny 28Â° /19Â° | 8 Cloudy 29Â° /20Â° | 9 Sunny 30Â° /20Â° | 10 Partly cloudy 31Â° /21Â° | 11 Sunny 31Â° /20Â° | 12 Sunny 30Â° /21Â° |'}]\", name='tavily_search_results_json', tool_call_id='call_9IKDlPoxdS3rHnNGtrw8lupx')]}\n",
      "{'messages': [AIMessage(content='The current weather in Los Angeles is:\\n\\n- **Daytime**: Partly cloudy with a high of around 79Â°F. \\n- **Nighttime**: Clear skies with a low of about 64Â°F.\\n\\nOverall, Los Angeles is experiencing a warm day with partly cloudy conditions.', response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 1975, 'total_tokens': 2034, 'prompt_tokens_details': {'cached_tokens': 1280, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'model_name': 'gpt-4o', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a9420d7b-7e7e-4fdd-9171-7a067a7a38b1-0')]}\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What about in la?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d93d149",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [AIMessage(content='Currently, Los Angeles is warmer than San Francisco. Los Angeles has a high of around 79Â°F, while San Francisco has a high of 66Â°F.', response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 2046, 'total_tokens': 2079, 'prompt_tokens_details': {'cached_tokens': 1920, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'model_name': 'gpt-4o', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e4387a0f-c8fa-45db-a7e2-f09bb7d15a02-0')]}\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "119f3fc3",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [AIMessage(content=\"Could you please clarify what you're comparing to determine which is warmer? Are you comparing two specific locations, types of clothing, materials, or something else? Let me know so I can provide the appropriate information.\", response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 149, 'total_tokens': 192, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_f9f4fb6dbf', 'finish_reason': 'stop', 'logprobs': None}, id='run-4fbf0c1f-a93c-4446-bfb2-c96be8936b3e-0')]}\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a29c1bdf",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
    "\n",
    "memory = AsyncSqliteSaver.from_conn_string(\":memory:\")\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c42964c7",
   "metadata": {
    "height": 200
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: This API is in beta and may change in the future.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'current weather in San Francisco'}, 'id': 'call_8etfZasfe0eUw99Q9qqzCQ0N'}\n",
      "Back to the model!\n",
      "The| current| weather| in| San| Francisco| is| cloudy|.| The| temperature| is| expected| to| reach| a| high| of| |66|Â°F| today|,| with| winds| from| the| west| at| |10| to| |20| mph|.| This| afternoon|,| the| skies| will| become| partly| cloudy|.| Tonight|,| the| skies| will| be| partly| cloudy|,| becoming| over|cast| overnight| with| a| low| of| |56|Â°F|.|"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in SF?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "async for event in abot.graph.astream_events({\"messages\": messages}, thread, version=\"v1\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        content = event[\"data\"][\"chunk\"].content\n",
    "        if content:\n",
    "            # Empty content in the context of OpenAI means\n",
    "            # that the model is asking for a tool to be invoked.\n",
    "            # So we only print non-empty content\n",
    "            print(content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007dd0f5",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
